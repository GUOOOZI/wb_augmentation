{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GUOOOZI/wb_augmentation/blob/main/WB_augmenter_on_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5T83Em81LgZ"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03Oyj9TE1Lgc"
      },
      "source": [
        "\n",
        "WB augmenter tutorial\n",
        "=====================\n",
        "\n",
        "This is a tutorial on how to use our WB augmenter to augment images on the fly. In this example, we are going to train a neural network for classification on the cifar10 dataset. For simplicity, we will use only 500 images per class for training.\n",
        "\n",
        "In particular, this tutorial focuses on showing how to use the WB augmenter with PyTorch built-in datasets, like cifar10.\n",
        "\n",
        "The approach used in this tutorial could also be used for custom datasets/dataLoaders. Another way to apply the WB augmenter on the fly to loaded images is also provided in the official github page: https://github.com/mahmoudnafifi/WB_color_augmenter\n",
        "\n",
        "In this tutorial, we clone only the Python version of the WB augmenter from here: https://github.com/mahmoudnafifi/WB_color_augmenter_python. To see more details and examples, please check the official github page.\n",
        "\n",
        "\n",
        "\n",
        "**Citation:**\n",
        "\n",
        "*Mahmoud Afifi and Michael S. Brown. What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance. International Conference on Computer Vision (ICCV), 2019.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt6K-wLC1Lgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f08e1ec-3291-425c-c4b1-37731cc3fba1"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from collections import defaultdict, deque\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Training/testing is going to use {device}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training/testing is going to use cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YbwX85y-WD6"
      },
      "source": [
        "0. Clone the WB augmenter repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdUFWMRx-fRB",
        "outputId": "2d98da77-7635-4802-8717-9bba0d4d8b9a"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "if os.path.exists('WB_color_augmenter_python') != 0:\n",
        "  shutil.rmtree('WB_color_augmenter_python')\n",
        "\n",
        "!git clone https://github.com/mahmoudnafifi/WB_color_augmenter_python.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'WB_color_augmenter_python'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 37 (delta 12), reused 21 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (37/37), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpGUG4-3-ieh"
      },
      "source": [
        "import WB_color_augmenter_python.WBEmulator as wb_aug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "831_i2OA1-gk"
      },
      "source": [
        "1. Dataset class with our data augmenter. See `__getitem__` that applies the WB augmentation on the fly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEfxf3Wr6pLg"
      },
      "source": [
        "class Cifar10_wo_WB_aug(datasets.CIFAR10):\n",
        "    def __init__(self, path, transforms, train=True, download=True):\n",
        "        super().__init__(path, train, download=download)\n",
        "        self.transforms = transforms\n",
        "        self.n_images_per_class = 500\n",
        "        self.n_classes = 10\n",
        "        self.new2old_indices = self.create_idx_mapping()\n",
        "\n",
        "    def create_idx_mapping(self):\n",
        "        label2idx = defaultdict(lambda: deque(maxlen=self.n_images_per_class))\n",
        "        for original_idx in range(super().__len__()):\n",
        "            _, label = super().__getitem__(original_idx)\n",
        "            label2idx[label].append(original_idx)\n",
        "\n",
        "        old_idxs = set(itertools.chain(*label2idx.values()))\n",
        "        new2old_indices = {}\n",
        "        for new_idx, old_idx in enumerate(old_idxs):\n",
        "            new2old_indices[new_idx] = old_idx\n",
        "\n",
        "        return new2old_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.new2old_indices)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.new2old_indices[index]\n",
        "        im, label = super().__getitem__(index)\n",
        "        return self.transforms(im), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwGITEjDMnxy"
      },
      "source": [
        "class Cifar10_w_WB_aug(datasets.CIFAR10):\n",
        "    def __init__(self, path, transforms, train=True, download=True):\n",
        "        super().__init__(path, train, download=download)\n",
        "        self.path = path\n",
        "        self.transforms = transforms\n",
        "        self.n_images_per_class = 500\n",
        "        self.n_classes = 10\n",
        "        self.new2old_indices = self.create_idx_mapping()\n",
        "        self.wb_color_aug = wb_aug.WBEmulator()\n",
        "        self.mapping = self.compute_mapping()\n",
        "\n",
        "    def create_idx_mapping(self):\n",
        "        label2idx = defaultdict(lambda: deque(maxlen=self.n_images_per_class))\n",
        "        for original_idx in range(super().__len__()):\n",
        "            _, label = super().__getitem__(original_idx)\n",
        "            label2idx[label].append(original_idx)\n",
        "\n",
        "        old_idxs = set(itertools.chain(*label2idx.values()))\n",
        "        new2old_indices = {}\n",
        "        for new_idx, old_idx in enumerate(old_idxs):\n",
        "            new2old_indices[new_idx] = old_idx\n",
        "\n",
        "        return new2old_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.new2old_indices)\n",
        "\n",
        "    def compute_mapping(self):\n",
        "        if os.path.exists(os.path.join(self.path, 'wb_mfs.pickle')):\n",
        "          with open(os.path.join(self.path, 'wb_mfs.pickle'), 'rb') as handle:\n",
        "            mapping_funcs = pickle.load(handle)\n",
        "          return mapping_funcs\n",
        "\n",
        "        print('Computing mapping functions for WB augmenter. '\n",
        "        'This process may take time....')\n",
        "        mapping_funcs = []\n",
        "        for idx in range(super().__len__()):\n",
        "            img, label = super().__getitem__(idx)\n",
        "            mfs = self.wb_color_aug.computeMappingFunc(img)\n",
        "            mapping_funcs.append(mfs)\n",
        "        with open(os.path.join(self.path, 'wb_mfs.pickle'), 'wb') as handle:\n",
        "          pickle.dump(mapping_funcs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        return mapping_funcs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.new2old_indices[index]\n",
        "        im, label = super().__getitem__(index)\n",
        "        mfs = self.mapping[index]\n",
        "        ind = np.random.randint(len(mfs))\n",
        "        mf = mfs[ind]\n",
        "        wb_aug.changeWB(np.array(im), mf)\n",
        "        return self.transforms(im), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TGgOHBD6qx3"
      },
      "source": [
        "2. Download dataset and create dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T226o1Jb64P3",
        "outputId": "cb97c5ab-b9f0-452c-9a59-4de2e4b4a835"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "## training loaders\n",
        "# w/ white-balance augmenter\n",
        "trainset_w_WB_aug = Cifar10_w_WB_aug(path = \"./data\", transforms=transform,\n",
        "                                     train=True, download=True)\n",
        "trainloader_w_WB_aug = torch.utils.data.DataLoader(\n",
        "    trainset_w_WB_aug, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# w/o white-balance augmenter\n",
        "trainset_wo_WB_aug = Cifar10_wo_WB_aug(path = \"./data\",transforms=transform,\n",
        "                                       train=True, download=True)\n",
        "trainloader_wo_WB_aug = torch.utils.data.DataLoader(\n",
        "    trainset_wo_WB_aug, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "## testing loaders\n",
        "# w/ white-balance augmenter\n",
        "\n",
        "testset = Cifar10_wo_WB_aug(path = \"./data\",transforms=transform,\n",
        "                                      train=False, download=True)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeknT5752BU-"
      },
      "source": [
        "3. Build network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgvOuhji1Lgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871b4515-8b3d-4844-e4c8-1bcda096720c"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 240)\n",
        "        self.fc2 = nn.Linear(240, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net_w_WB_aug = Net()\n",
        "net_w_WB_aug.to(device)\n",
        "\n",
        "net_wo_WB_aug = Net()\n",
        "net_wo_WB_aug.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=240, bias=True)\n",
              "  (fc2): Linear(in_features=240, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzRbZoYC1Lgl"
      },
      "source": [
        "4. Loss function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWywzV7R1Lgo"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_w_WB_aug = optim.SGD(net_w_WB_aug.parameters(), lr=0.001,\n",
        "                               momentum=0.9)\n",
        "optimizer_wo_WB_aug = optim.SGD(net_wo_WB_aug.parameters(), lr=0.001,\n",
        "                                momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcvWK9eM1Lgo"
      },
      "source": [
        "5. Train two networks: one with the WB augmenter and the second one without the WB augmenter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQLON9Ar1Lgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074f2d73-0490-4b51-ba9c-a872170a4990"
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "## train w/ the WB augmenter\n",
        "print('Training w/ WB augmentation')\n",
        "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader_w_WB_aug, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer_w_WB_aug.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net_w_WB_aug(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_w_WB_aug.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 50 == 0:    # print every 50 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "PATH = './cifar_net_w_WB_aug.pth'\n",
        "torch.save(net_w_WB_aug.state_dict(), PATH)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## train w/o the WB augmenter\n",
        "print('\\n\\n\\nTraining w/ WB augmentation')\n",
        "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader_wo_WB_aug, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer_wo_WB_aug.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net_wo_WB_aug(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_wo_WB_aug.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 50 == 0:    # print every 50 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "PATH = './cifar_net_wo_WB_aug.pth'\n",
        "torch.save(net_wo_WB_aug.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training w/ WB augmentation\n",
            "[1,    50] loss: 2.308\n",
            "[1,   100] loss: 2.303\n",
            "[1,   150] loss: 2.304\n",
            "[2,    50] loss: 2.303\n",
            "[2,   100] loss: 2.303\n",
            "[2,   150] loss: 2.304\n",
            "[3,    50] loss: 2.302\n",
            "[3,   100] loss: 2.301\n",
            "[3,   150] loss: 2.304\n",
            "[4,    50] loss: 2.301\n",
            "[4,   100] loss: 2.302\n",
            "[4,   150] loss: 2.301\n",
            "[5,    50] loss: 2.300\n",
            "[5,   100] loss: 2.300\n",
            "[5,   150] loss: 2.300\n",
            "[6,    50] loss: 2.298\n",
            "[6,   100] loss: 2.298\n",
            "[6,   150] loss: 2.298\n",
            "[7,    50] loss: 2.295\n",
            "[7,   100] loss: 2.296\n",
            "[7,   150] loss: 2.294\n",
            "[8,    50] loss: 2.291\n",
            "[8,   100] loss: 2.290\n",
            "[8,   150] loss: 2.286\n",
            "[9,    50] loss: 2.281\n",
            "[9,   100] loss: 2.274\n",
            "[9,   150] loss: 2.266\n",
            "[10,    50] loss: 2.248\n",
            "[10,   100] loss: 2.240\n",
            "[10,   150] loss: 2.220\n",
            "[11,    50] loss: 2.199\n",
            "[11,   100] loss: 2.182\n",
            "[11,   150] loss: 2.176\n",
            "[12,    50] loss: 2.138\n",
            "[12,   100] loss: 2.143\n",
            "[12,   150] loss: 2.102\n",
            "[13,    50] loss: 2.091\n",
            "[13,   100] loss: 2.083\n",
            "[13,   150] loss: 2.046\n",
            "[14,    50] loss: 2.043\n",
            "[14,   100] loss: 2.054\n",
            "[14,   150] loss: 2.012\n",
            "[15,    50] loss: 2.012\n",
            "[15,   100] loss: 2.002\n",
            "[15,   150] loss: 1.993\n",
            "[16,    50] loss: 1.986\n",
            "[16,   100] loss: 1.971\n",
            "[16,   150] loss: 1.948\n",
            "[17,    50] loss: 1.955\n",
            "[17,   100] loss: 1.928\n",
            "[17,   150] loss: 1.918\n",
            "[18,    50] loss: 1.892\n",
            "[18,   100] loss: 1.886\n",
            "[18,   150] loss: 1.872\n",
            "[19,    50] loss: 1.835\n",
            "[19,   100] loss: 1.839\n",
            "[19,   150] loss: 1.833\n",
            "[20,    50] loss: 1.792\n",
            "[20,   100] loss: 1.795\n",
            "[20,   150] loss: 1.783\n",
            "[21,    50] loss: 1.752\n",
            "[21,   100] loss: 1.733\n",
            "[21,   150] loss: 1.752\n",
            "[22,    50] loss: 1.735\n",
            "[22,   100] loss: 1.686\n",
            "[22,   150] loss: 1.697\n",
            "[23,    50] loss: 1.643\n",
            "[23,   100] loss: 1.679\n",
            "[23,   150] loss: 1.707\n",
            "[24,    50] loss: 1.678\n",
            "[24,   100] loss: 1.635\n",
            "[24,   150] loss: 1.627\n",
            "[25,    50] loss: 1.629\n",
            "[25,   100] loss: 1.602\n",
            "[25,   150] loss: 1.636\n",
            "[26,    50] loss: 1.618\n",
            "[26,   100] loss: 1.575\n",
            "[26,   150] loss: 1.584\n",
            "[27,    50] loss: 1.589\n",
            "[27,   100] loss: 1.544\n",
            "[27,   150] loss: 1.580\n",
            "[28,    50] loss: 1.533\n",
            "[28,   100] loss: 1.544\n",
            "[28,   150] loss: 1.566\n",
            "[29,    50] loss: 1.556\n",
            "[29,   100] loss: 1.522\n",
            "[29,   150] loss: 1.527\n",
            "[30,    50] loss: 1.501\n",
            "[30,   100] loss: 1.518\n",
            "[30,   150] loss: 1.516\n",
            "[31,    50] loss: 1.491\n",
            "[31,   100] loss: 1.508\n",
            "[31,   150] loss: 1.475\n",
            "[32,    50] loss: 1.464\n",
            "[32,   100] loss: 1.472\n",
            "[32,   150] loss: 1.512\n",
            "[33,    50] loss: 1.476\n",
            "[33,   100] loss: 1.490\n",
            "[33,   150] loss: 1.431\n",
            "[34,    50] loss: 1.468\n",
            "[34,   100] loss: 1.428\n",
            "[34,   150] loss: 1.431\n",
            "[35,    50] loss: 1.394\n",
            "[35,   100] loss: 1.448\n",
            "[35,   150] loss: 1.432\n",
            "[36,    50] loss: 1.422\n",
            "[36,   100] loss: 1.392\n",
            "[36,   150] loss: 1.395\n",
            "[37,    50] loss: 1.371\n",
            "[37,   100] loss: 1.379\n",
            "[37,   150] loss: 1.412\n",
            "[38,    50] loss: 1.363\n",
            "[38,   100] loss: 1.371\n",
            "[38,   150] loss: 1.373\n",
            "[39,    50] loss: 1.342\n",
            "[39,   100] loss: 1.310\n",
            "[39,   150] loss: 1.370\n",
            "[40,    50] loss: 1.333\n",
            "[40,   100] loss: 1.310\n",
            "[40,   150] loss: 1.321\n",
            "[41,    50] loss: 1.325\n",
            "[41,   100] loss: 1.298\n",
            "[41,   150] loss: 1.278\n",
            "[42,    50] loss: 1.311\n",
            "[42,   100] loss: 1.257\n",
            "[42,   150] loss: 1.298\n",
            "[43,    50] loss: 1.258\n",
            "[43,   100] loss: 1.260\n",
            "[43,   150] loss: 1.279\n",
            "[44,    50] loss: 1.205\n",
            "[44,   100] loss: 1.257\n",
            "[44,   150] loss: 1.295\n",
            "[45,    50] loss: 1.214\n",
            "[45,   100] loss: 1.228\n",
            "[45,   150] loss: 1.236\n",
            "[46,    50] loss: 1.180\n",
            "[46,   100] loss: 1.221\n",
            "[46,   150] loss: 1.178\n",
            "[47,    50] loss: 1.157\n",
            "[47,   100] loss: 1.176\n",
            "[47,   150] loss: 1.207\n",
            "[48,    50] loss: 1.160\n",
            "[48,   100] loss: 1.176\n",
            "[48,   150] loss: 1.132\n",
            "[49,    50] loss: 1.194\n",
            "[49,   100] loss: 1.107\n",
            "[49,   150] loss: 1.137\n",
            "[50,    50] loss: 1.086\n",
            "[50,   100] loss: 1.139\n",
            "[50,   150] loss: 1.096\n",
            "[51,    50] loss: 1.073\n",
            "[51,   100] loss: 1.128\n",
            "[51,   150] loss: 1.047\n",
            "[52,    50] loss: 1.051\n",
            "[52,   100] loss: 1.020\n",
            "[52,   150] loss: 1.098\n",
            "[53,    50] loss: 1.017\n",
            "[53,   100] loss: 0.999\n",
            "[53,   150] loss: 1.031\n",
            "[54,    50] loss: 1.006\n",
            "[54,   100] loss: 1.043\n",
            "[54,   150] loss: 0.976\n",
            "[55,    50] loss: 0.937\n",
            "[55,   100] loss: 0.950\n",
            "[55,   150] loss: 1.031\n",
            "[56,    50] loss: 0.916\n",
            "[56,   100] loss: 0.949\n",
            "[56,   150] loss: 0.949\n",
            "[57,    50] loss: 0.899\n",
            "[57,   100] loss: 0.941\n",
            "[57,   150] loss: 0.910\n",
            "[58,    50] loss: 0.842\n",
            "[58,   100] loss: 0.867\n",
            "[58,   150] loss: 0.884\n",
            "[59,    50] loss: 0.838\n",
            "[59,   100] loss: 0.834\n",
            "[59,   150] loss: 0.858\n",
            "[60,    50] loss: 0.792\n",
            "[60,   100] loss: 0.851\n",
            "[60,   150] loss: 0.847\n",
            "[61,    50] loss: 0.723\n",
            "[61,   100] loss: 0.768\n",
            "[61,   150] loss: 0.848\n",
            "[62,    50] loss: 0.720\n",
            "[62,   100] loss: 0.750\n",
            "[62,   150] loss: 0.739\n",
            "[63,    50] loss: 0.705\n",
            "[63,   100] loss: 0.646\n",
            "[63,   150] loss: 0.727\n",
            "[64,    50] loss: 0.647\n",
            "[64,   100] loss: 0.686\n",
            "[64,   150] loss: 0.688\n",
            "[65,    50] loss: 0.598\n",
            "[65,   100] loss: 0.649\n",
            "[65,   150] loss: 0.670\n",
            "[66,    50] loss: 0.580\n",
            "[66,   100] loss: 0.609\n",
            "[66,   150] loss: 0.605\n",
            "[67,    50] loss: 0.585\n",
            "[67,   100] loss: 0.561\n",
            "[67,   150] loss: 0.571\n",
            "[68,    50] loss: 0.507\n",
            "[68,   100] loss: 0.539\n",
            "[68,   150] loss: 0.538\n",
            "[69,    50] loss: 0.509\n",
            "[69,   100] loss: 0.514\n",
            "[69,   150] loss: 0.474\n",
            "[70,    50] loss: 0.408\n",
            "[70,   100] loss: 0.452\n",
            "[70,   150] loss: 0.498\n",
            "[71,    50] loss: 0.416\n",
            "[71,   100] loss: 0.389\n",
            "[71,   150] loss: 0.430\n",
            "[72,    50] loss: 0.378\n",
            "[72,   100] loss: 0.371\n",
            "[72,   150] loss: 0.380\n",
            "[73,    50] loss: 0.359\n",
            "[73,   100] loss: 0.297\n",
            "[73,   150] loss: 0.404\n",
            "[74,    50] loss: 0.337\n",
            "[74,   100] loss: 0.342\n",
            "[74,   150] loss: 0.331\n",
            "[75,    50] loss: 0.271\n",
            "[75,   100] loss: 0.270\n",
            "[75,   150] loss: 0.312\n",
            "[76,    50] loss: 0.256\n",
            "[76,   100] loss: 0.226\n",
            "[76,   150] loss: 0.261\n",
            "[77,    50] loss: 0.236\n",
            "[77,   100] loss: 0.233\n",
            "[77,   150] loss: 0.234\n",
            "[78,    50] loss: 0.187\n",
            "[78,   100] loss: 0.163\n",
            "[78,   150] loss: 0.223\n",
            "[79,    50] loss: 0.178\n",
            "[79,   100] loss: 0.153\n",
            "[79,   150] loss: 0.169\n",
            "[80,    50] loss: 0.154\n",
            "[80,   100] loss: 0.140\n",
            "[80,   150] loss: 0.175\n",
            "[81,    50] loss: 0.116\n",
            "[81,   100] loss: 0.130\n",
            "[81,   150] loss: 0.109\n",
            "[82,    50] loss: 0.100\n",
            "[82,   100] loss: 0.103\n",
            "[82,   150] loss: 0.112\n",
            "[83,    50] loss: 0.143\n",
            "[83,   100] loss: 0.113\n",
            "[83,   150] loss: 0.121\n",
            "[84,    50] loss: 0.157\n",
            "[84,   100] loss: 0.149\n",
            "[84,   150] loss: 0.095\n",
            "[85,    50] loss: 0.120\n",
            "[85,   100] loss: 0.093\n",
            "[85,   150] loss: 0.088\n",
            "[86,    50] loss: 0.066\n",
            "[86,   100] loss: 0.055\n",
            "[86,   150] loss: 0.074\n",
            "[87,    50] loss: 0.062\n",
            "[87,   100] loss: 0.056\n",
            "[87,   150] loss: 0.053\n",
            "[88,    50] loss: 0.040\n",
            "[88,   100] loss: 0.040\n",
            "[88,   150] loss: 0.042\n",
            "[89,    50] loss: 0.101\n",
            "[89,   100] loss: 0.055\n",
            "[89,   150] loss: 0.042\n",
            "[90,    50] loss: 0.031\n",
            "[90,   100] loss: 0.026\n",
            "[90,   150] loss: 0.024\n",
            "[91,    50] loss: 0.021\n",
            "[91,   100] loss: 0.019\n",
            "[91,   150] loss: 0.020\n",
            "[92,    50] loss: 0.016\n",
            "[92,   100] loss: 0.017\n",
            "[92,   150] loss: 0.018\n",
            "[93,    50] loss: 0.015\n",
            "[93,   100] loss: 0.017\n",
            "[93,   150] loss: 0.017\n",
            "[94,    50] loss: 0.013\n",
            "[94,   100] loss: 0.013\n",
            "[94,   150] loss: 0.014\n",
            "[95,    50] loss: 0.014\n",
            "[95,   100] loss: 0.012\n",
            "[95,   150] loss: 0.011\n",
            "[96,    50] loss: 0.011\n",
            "[96,   100] loss: 0.010\n",
            "[96,   150] loss: 0.011\n",
            "[97,    50] loss: 0.010\n",
            "[97,   100] loss: 0.010\n",
            "[97,   150] loss: 0.009\n",
            "[98,    50] loss: 0.009\n",
            "[98,   100] loss: 0.009\n",
            "[98,   150] loss: 0.009\n",
            "[99,    50] loss: 0.008\n",
            "[99,   100] loss: 0.008\n",
            "[99,   150] loss: 0.008\n",
            "[100,    50] loss: 0.007\n",
            "[100,   100] loss: 0.008\n",
            "[100,   150] loss: 0.008\n",
            "Finished Training\n",
            "\n",
            "\n",
            "\n",
            "Training w/ WB augmentation\n",
            "[1,    50] loss: 2.302\n",
            "[1,   100] loss: 2.304\n",
            "[1,   150] loss: 2.305\n",
            "[2,    50] loss: 2.301\n",
            "[2,   100] loss: 2.301\n",
            "[2,   150] loss: 2.300\n",
            "[3,    50] loss: 2.298\n",
            "[3,   100] loss: 2.296\n",
            "[3,   150] loss: 2.293\n",
            "[4,    50] loss: 2.288\n",
            "[4,   100] loss: 2.282\n",
            "[4,   150] loss: 2.279\n",
            "[5,    50] loss: 2.258\n",
            "[5,   100] loss: 2.267\n",
            "[5,   150] loss: 2.242\n",
            "[6,    50] loss: 2.239\n",
            "[6,   100] loss: 2.228\n",
            "[6,   150] loss: 2.214\n",
            "[7,    50] loss: 2.205\n",
            "[7,   100] loss: 2.195\n",
            "[7,   150] loss: 2.179\n",
            "[8,    50] loss: 2.177\n",
            "[8,   100] loss: 2.156\n",
            "[8,   150] loss: 2.153\n",
            "[9,    50] loss: 2.142\n",
            "[9,   100] loss: 2.135\n",
            "[9,   150] loss: 2.127\n",
            "[10,    50] loss: 2.118\n",
            "[10,   100] loss: 2.128\n",
            "[10,   150] loss: 2.081\n",
            "[11,    50] loss: 2.104\n",
            "[11,   100] loss: 2.071\n",
            "[11,   150] loss: 2.085\n",
            "[12,    50] loss: 2.069\n",
            "[12,   100] loss: 2.062\n",
            "[12,   150] loss: 2.051\n",
            "[13,    50] loss: 2.044\n",
            "[13,   100] loss: 2.036\n",
            "[13,   150] loss: 2.006\n",
            "[14,    50] loss: 2.003\n",
            "[14,   100] loss: 2.006\n",
            "[14,   150] loss: 1.988\n",
            "[15,    50] loss: 2.002\n",
            "[15,   100] loss: 1.971\n",
            "[15,   150] loss: 1.948\n",
            "[16,    50] loss: 1.938\n",
            "[16,   100] loss: 1.953\n",
            "[16,   150] loss: 1.957\n",
            "[17,    50] loss: 1.925\n",
            "[17,   100] loss: 1.901\n",
            "[17,   150] loss: 1.948\n",
            "[18,    50] loss: 1.893\n",
            "[18,   100] loss: 1.896\n",
            "[18,   150] loss: 1.897\n",
            "[19,    50] loss: 1.860\n",
            "[19,   100] loss: 1.878\n",
            "[19,   150] loss: 1.867\n",
            "[20,    50] loss: 1.820\n",
            "[20,   100] loss: 1.858\n",
            "[20,   150] loss: 1.829\n",
            "[21,    50] loss: 1.813\n",
            "[21,   100] loss: 1.810\n",
            "[21,   150] loss: 1.778\n",
            "[22,    50] loss: 1.773\n",
            "[22,   100] loss: 1.795\n",
            "[22,   150] loss: 1.754\n",
            "[23,    50] loss: 1.727\n",
            "[23,   100] loss: 1.719\n",
            "[23,   150] loss: 1.763\n",
            "[24,    50] loss: 1.688\n",
            "[24,   100] loss: 1.713\n",
            "[24,   150] loss: 1.698\n",
            "[25,    50] loss: 1.696\n",
            "[25,   100] loss: 1.686\n",
            "[25,   150] loss: 1.656\n",
            "[26,    50] loss: 1.626\n",
            "[26,   100] loss: 1.643\n",
            "[26,   150] loss: 1.651\n",
            "[27,    50] loss: 1.629\n",
            "[27,   100] loss: 1.622\n",
            "[27,   150] loss: 1.615\n",
            "[28,    50] loss: 1.597\n",
            "[28,   100] loss: 1.594\n",
            "[28,   150] loss: 1.578\n",
            "[29,    50] loss: 1.580\n",
            "[29,   100] loss: 1.566\n",
            "[29,   150] loss: 1.530\n",
            "[30,    50] loss: 1.551\n",
            "[30,   100] loss: 1.503\n",
            "[30,   150] loss: 1.556\n",
            "[31,    50] loss: 1.522\n",
            "[31,   100] loss: 1.552\n",
            "[31,   150] loss: 1.510\n",
            "[32,    50] loss: 1.526\n",
            "[32,   100] loss: 1.475\n",
            "[32,   150] loss: 1.469\n",
            "[33,    50] loss: 1.452\n",
            "[33,   100] loss: 1.498\n",
            "[33,   150] loss: 1.448\n",
            "[34,    50] loss: 1.461\n",
            "[34,   100] loss: 1.426\n",
            "[34,   150] loss: 1.437\n",
            "[35,    50] loss: 1.425\n",
            "[35,   100] loss: 1.414\n",
            "[35,   150] loss: 1.452\n",
            "[36,    50] loss: 1.368\n",
            "[36,   100] loss: 1.420\n",
            "[36,   150] loss: 1.442\n",
            "[37,    50] loss: 1.394\n",
            "[37,   100] loss: 1.389\n",
            "[37,   150] loss: 1.377\n",
            "[38,    50] loss: 1.337\n",
            "[38,   100] loss: 1.383\n",
            "[38,   150] loss: 1.423\n",
            "[39,    50] loss: 1.327\n",
            "[39,   100] loss: 1.349\n",
            "[39,   150] loss: 1.372\n",
            "[40,    50] loss: 1.294\n",
            "[40,   100] loss: 1.368\n",
            "[40,   150] loss: 1.306\n",
            "[41,    50] loss: 1.295\n",
            "[41,   100] loss: 1.304\n",
            "[41,   150] loss: 1.315\n",
            "[42,    50] loss: 1.306\n",
            "[42,   100] loss: 1.292\n",
            "[42,   150] loss: 1.293\n",
            "[43,    50] loss: 1.261\n",
            "[43,   100] loss: 1.257\n",
            "[43,   150] loss: 1.283\n",
            "[44,    50] loss: 1.227\n",
            "[44,   100] loss: 1.290\n",
            "[44,   150] loss: 1.255\n",
            "[45,    50] loss: 1.257\n",
            "[45,   100] loss: 1.235\n",
            "[45,   150] loss: 1.191\n",
            "[46,    50] loss: 1.223\n",
            "[46,   100] loss: 1.182\n",
            "[46,   150] loss: 1.225\n",
            "[47,    50] loss: 1.180\n",
            "[47,   100] loss: 1.196\n",
            "[47,   150] loss: 1.160\n",
            "[48,    50] loss: 1.119\n",
            "[48,   100] loss: 1.170\n",
            "[48,   150] loss: 1.187\n",
            "[49,    50] loss: 1.085\n",
            "[49,   100] loss: 1.170\n",
            "[49,   150] loss: 1.176\n",
            "[50,    50] loss: 1.099\n",
            "[50,   100] loss: 1.127\n",
            "[50,   150] loss: 1.116\n",
            "[51,    50] loss: 1.033\n",
            "[51,   100] loss: 1.120\n",
            "[51,   150] loss: 1.125\n",
            "[52,    50] loss: 1.029\n",
            "[52,   100] loss: 1.094\n",
            "[52,   150] loss: 1.068\n",
            "[53,    50] loss: 1.044\n",
            "[53,   100] loss: 1.057\n",
            "[53,   150] loss: 1.060\n",
            "[54,    50] loss: 1.029\n",
            "[54,   100] loss: 1.052\n",
            "[54,   150] loss: 1.021\n",
            "[55,    50] loss: 0.996\n",
            "[55,   100] loss: 1.011\n",
            "[55,   150] loss: 0.998\n",
            "[56,    50] loss: 0.952\n",
            "[56,   100] loss: 0.976\n",
            "[56,   150] loss: 0.969\n",
            "[57,    50] loss: 0.927\n",
            "[57,   100] loss: 0.897\n",
            "[57,   150] loss: 0.975\n",
            "[58,    50] loss: 0.855\n",
            "[58,   100] loss: 0.931\n",
            "[58,   150] loss: 0.977\n",
            "[59,    50] loss: 0.870\n",
            "[59,   100] loss: 0.857\n",
            "[59,   150] loss: 0.956\n",
            "[60,    50] loss: 0.856\n",
            "[60,   100] loss: 0.835\n",
            "[60,   150] loss: 0.865\n",
            "[61,    50] loss: 0.802\n",
            "[61,   100] loss: 0.803\n",
            "[61,   150] loss: 0.840\n",
            "[62,    50] loss: 0.774\n",
            "[62,   100] loss: 0.789\n",
            "[62,   150] loss: 0.813\n",
            "[63,    50] loss: 0.792\n",
            "[63,   100] loss: 0.782\n",
            "[63,   150] loss: 0.739\n",
            "[64,    50] loss: 0.707\n",
            "[64,   100] loss: 0.713\n",
            "[64,   150] loss: 0.759\n",
            "[65,    50] loss: 0.655\n",
            "[65,   100] loss: 0.681\n",
            "[65,   150] loss: 0.703\n",
            "[66,    50] loss: 0.624\n",
            "[66,   100] loss: 0.652\n",
            "[66,   150] loss: 0.656\n",
            "[67,    50] loss: 0.610\n",
            "[67,   100] loss: 0.664\n",
            "[67,   150] loss: 0.663\n",
            "[68,    50] loss: 0.591\n",
            "[68,   100] loss: 0.625\n",
            "[68,   150] loss: 0.598\n",
            "[69,    50] loss: 0.533\n",
            "[69,   100] loss: 0.567\n",
            "[69,   150] loss: 0.614\n",
            "[70,    50] loss: 0.491\n",
            "[70,   100] loss: 0.508\n",
            "[70,   150] loss: 0.560\n",
            "[71,    50] loss: 0.447\n",
            "[71,   100] loss: 0.480\n",
            "[71,   150] loss: 0.517\n",
            "[72,    50] loss: 0.444\n",
            "[72,   100] loss: 0.488\n",
            "[72,   150] loss: 0.496\n",
            "[73,    50] loss: 0.400\n",
            "[73,   100] loss: 0.413\n",
            "[73,   150] loss: 0.449\n",
            "[74,    50] loss: 0.368\n",
            "[74,   100] loss: 0.378\n",
            "[74,   150] loss: 0.442\n",
            "[75,    50] loss: 0.432\n",
            "[75,   100] loss: 0.441\n",
            "[75,   150] loss: 0.390\n",
            "[76,    50] loss: 0.334\n",
            "[76,   100] loss: 0.360\n",
            "[76,   150] loss: 0.412\n",
            "[77,    50] loss: 0.391\n",
            "[77,   100] loss: 0.337\n",
            "[77,   150] loss: 0.359\n",
            "[78,    50] loss: 0.323\n",
            "[78,   100] loss: 0.285\n",
            "[78,   150] loss: 0.303\n",
            "[79,    50] loss: 0.245\n",
            "[79,   100] loss: 0.275\n",
            "[79,   150] loss: 0.291\n",
            "[80,    50] loss: 0.217\n",
            "[80,   100] loss: 0.275\n",
            "[80,   150] loss: 0.272\n",
            "[81,    50] loss: 0.227\n",
            "[81,   100] loss: 0.208\n",
            "[81,   150] loss: 0.258\n",
            "[82,    50] loss: 0.198\n",
            "[82,   100] loss: 0.221\n",
            "[82,   150] loss: 0.247\n",
            "[83,    50] loss: 0.175\n",
            "[83,   100] loss: 0.181\n",
            "[83,   150] loss: 0.179\n",
            "[84,    50] loss: 0.201\n",
            "[84,   100] loss: 0.233\n",
            "[84,   150] loss: 0.191\n",
            "[85,    50] loss: 0.139\n",
            "[85,   100] loss: 0.150\n",
            "[85,   150] loss: 0.154\n",
            "[86,    50] loss: 0.107\n",
            "[86,   100] loss: 0.143\n",
            "[86,   150] loss: 0.139\n",
            "[87,    50] loss: 0.127\n",
            "[87,   100] loss: 0.122\n",
            "[87,   150] loss: 0.143\n",
            "[88,    50] loss: 0.219\n",
            "[88,   100] loss: 0.143\n",
            "[88,   150] loss: 0.134\n",
            "[89,    50] loss: 0.134\n",
            "[89,   100] loss: 0.165\n",
            "[89,   150] loss: 0.151\n",
            "[90,    50] loss: 0.143\n",
            "[90,   100] loss: 0.153\n",
            "[90,   150] loss: 0.227\n",
            "[91,    50] loss: 0.180\n",
            "[91,   100] loss: 0.174\n",
            "[91,   150] loss: 0.242\n",
            "[92,    50] loss: 0.154\n",
            "[92,   100] loss: 0.119\n",
            "[92,   150] loss: 0.130\n",
            "[93,    50] loss: 0.071\n",
            "[93,   100] loss: 0.058\n",
            "[93,   150] loss: 0.053\n",
            "[94,    50] loss: 0.045\n",
            "[94,   100] loss: 0.045\n",
            "[94,   150] loss: 0.046\n",
            "[95,    50] loss: 0.034\n",
            "[95,   100] loss: 0.029\n",
            "[95,   150] loss: 0.029\n",
            "[96,    50] loss: 0.022\n",
            "[96,   100] loss: 0.021\n",
            "[96,   150] loss: 0.025\n",
            "[97,    50] loss: 0.019\n",
            "[97,   100] loss: 0.021\n",
            "[97,   150] loss: 0.018\n",
            "[98,    50] loss: 0.014\n",
            "[98,   100] loss: 0.016\n",
            "[98,   150] loss: 0.017\n",
            "[99,    50] loss: 0.014\n",
            "[99,   100] loss: 0.014\n",
            "[99,   150] loss: 0.014\n",
            "[100,    50] loss: 0.010\n",
            "[100,   100] loss: 0.013\n",
            "[100,   150] loss: 0.013\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVU7W47G1Lgq"
      },
      "source": [
        "6. Test our networks on the original test set and the test set with some white-balance degradations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wemYuh4g1Lgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe1b4627-ee50-488d-8d02-790862c121e2"
      },
      "source": [
        "### test on original test set\n",
        "\n",
        "correct_wo_WB = 0\n",
        "correct_w_WB = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs_wo_WB_aug = net_wo_WB_aug(images)\n",
        "        outputs_w_WB_aug = net_w_WB_aug(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs_wo_WB_aug.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct_wo_WB += (predicted == labels).sum().item()\n",
        "\n",
        "        _, predicted = torch.max(outputs_w_WB_aug.data, 1)\n",
        "        correct_w_WB += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network trained without the WB augmenter on the 10000 '\n",
        "'test images: %d %%' % (100 * correct_wo_WB / total))\n",
        "\n",
        "print('Accuracy of the network trained with the WB augmenter on the 10000 '\n",
        "'test images: %d %%' % (100 * correct_w_WB / total))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network trained without the WB augmenter on the 10000 test images: 44 %\n",
            "Accuracy of the network trained with the WB augmenter on the 10000 test images: 47 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}